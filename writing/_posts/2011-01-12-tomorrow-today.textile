---
layout: post
title: Tomorrow, Today
published: false
---

Given our day-to-day relationship with computers, it can sometimes be difficult to remember the complexity of the tools that we use. Most of that feeling is, at this point, by design.

This comes from two places. One is computer science itself, where abstraction is a powerful tool for making machines more programmable. The history of computing machines is in part a history of adding layers of abstraction to them. Close to the machine-level, computers can only perform a fairly small set of very straight-forward tasks: move these bits from this spot to that, if these bits represent a value less than these bits, jump to this instruction, add these two sets of bits and store the resulting bits over here. And that level is already an abstraction, a way to communicate on a more human-understandable level what a computer should do.

Using a simple set of operations like those, it is possible to perform a lot of complex tasks, because the computer can perform each instruction at breath-taking speed. As computers have become faster, we have given them more complex tasks to perform, because they can handle them in an amount of time that we find comfortable at human scale. As these tasks have become more complex, we have made increasingly abstracted languages to communicate instructions to computers, so that at our human scale we can grapple more effectively with what a computer is doing. One line of code in a modern programming language could easily translate to thousands of instructions at the machine level.

<aside>There is still a lot of low-level programming going on, of course, but there is an absolutely explosive amount of high-level code being written all the time. In part, the increasing readability and expressiveness of code has given it broader appeal as something to learn.</aside>

So, we have spent much of the history of computing finding ways for programmers to more easily write instructions for them, while at the same time increasing the complexity of what the machine does compared to what a programmer typically writes. Abstraction gives computer programmers a way to grapple with the immense complexities of computing machines without having to think about all of the little operations going on behind the scenes.

The other source of our lack of mindfulness about the complexity of the tools we use is the campaign since the mid-1980s to make computers more user-friendly. The famous early champion of this kind of thinking is of course the Macintosh, which brought to the consumer market the graphical user interface for the first time. The graphical user interface is the paradigm by which most of us use our computers to this day, with a mouse controlling the location of a pointer on screen, and windows, menus, icons, folders, scrollbars, buttons... the metaphors go on. These visual metaphors serve as yet another powerful abstraction, and significantly lull us into a sense of simplicity. This sense is undoubtedly real. When I'm reading a website and see a link that interests me, I point to it, click on it, and I am presented with the page that link referred me to. That is simple from my point of view. My computer worked in concert with however many other computers over a network in order to deliver this result, which I had to think about not one bit.

All of this comes at something of a risk, however. At a relatively benign level, for computer users who know nothing of computer science, the simplifications carry the risk of creating a sense that the computer has a mind of its own, or is somehow performing magic. I'm not of the opinion that most non-programmers actually think that computers are capable of thinking for themselves or performing feats of magic. But at the levels of complexity at which modern computers operate, these ideas serve as the most similar concept.

At a more prominent level, we hand over increasing numbers of tasks which bear some risks to machines that can perform calculations we would never have been able to compute without them. Two that come to mind are helping us sort information and helping us make decisions about what to do with large amounts of money.

Personally, I am encouraged to see that the library sciences and the information sciences seem to be merging. Computers can, given proper instructions and data, do a good job of guessing whether two things might be relevant to each other. However, I think there is undoubtedly still a place for human-level expertise which does not always operate on a level reducible to how many links an article gets in the blogosphere. The human capacity for creativity also allows an extension into allowing for new possibilities that is difficult to model with computers without having a talented team of humans at hand to recalibrate it as the years march on.

On the financial side, we have already seen computational modeling play a part in enabling banks to take risks based on complex, perhaps overly-complex, reasoning. This kind of thing should serve as a warning sign to proceed with caution when dealing with decisions made by complex machines given instructions by abstract thinkers, and to remain mindful of just how much must really be going on inside our sophisticated tools.